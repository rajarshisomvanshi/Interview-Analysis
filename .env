# .env.example - Copy to .env and configure

# LLM Configuration (Default is local Ollama)
LLM_PROVIDER=ollama
LLM_MODEL=qwen2.5:1.5b
LLM_TEMPERATURE=0.3
LLM_MAX_TOKENS=1000

# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434

# API Keys (Optional if using OpenAI/Anthropic/Gemini)
# OPENAI_API_KEY=your_openai_api_key_here
# ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Model Paths
YOLO_PHONE_MODEL=yolov8n.pt
YOLO_CCTV_MODEL=yolov8s-pose.pt
WHISPER_MODEL=base
FACE_RECOGNITION_BACKEND=facenet
FACE_MESH_BACKEND=mediapipe

# Processing Parameters
VIDEO_FPS=30
AUDIO_SAMPLE_RATE=16000
FACE_CONFIDENCE_THRESHOLD=0.7
BODY_CONFIDENCE_THRESHOLD=0.5

# Storage
DATA_DIR=./data
VIDEO_RETENTION_DAYS=7
ENABLE_VIDEO_STORAGE=true

# API
API_HOST=0.0.0.0
API_PORT=8000
ENABLE_CORS=true

# Performance
USE_GPU=false
BATCH_SIZE=8
NUM_WORKERS=4

# Logging
LOG_LEVEL=INFO
SAVE_DEBUG_FRAMES=false
