# .env.example - Copy to .env and configure

# LLM Configuration
LLM_PROVIDER=openai  # openai, anthropic, or ollama
LLM_MODEL=gpt-4
LLM_TEMPERATURE=0.3
LLM_MAX_TOKENS=1000

# API Keys
OPENAI_API_KEY=your_openai_api_key_here
# ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Ollama Configuration (if using ollama)
OLLAMA_BASE_URL=http://localhost:11434

# Model Paths
YOLO_PHONE_MODEL=yolov8n.pt
YOLO_CCTV_MODEL=yolov8s-pose.pt
WHISPER_MODEL=base
FACE_RECOGNITION_BACKEND=facenet
FACE_MESH_BACKEND=mediapipe

# Processing Parameters
VIDEO_FPS=30
AUDIO_SAMPLE_RATE=16000
FACE_CONFIDENCE_THRESHOLD=0.7
BODY_CONFIDENCE_THRESHOLD=0.5

# Storage
DATA_DIR=./data
VIDEO_RETENTION_DAYS=7
ENABLE_VIDEO_STORAGE=true

# API
API_HOST=0.0.0.0
API_PORT=8000
ENABLE_CORS=true

# Performance
USE_GPU=true
BATCH_SIZE=8
NUM_WORKERS=4

# Logging
LOG_LEVEL=INFO
SAVE_DEBUG_FRAMES=false
